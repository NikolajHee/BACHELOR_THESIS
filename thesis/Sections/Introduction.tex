



\textcolor{red}{There can be multiple purposes of machine unlearning}
\begin{enumerate}
    \item Protecting privacy
    \item Relieving bias
    \item removing outdated data
\end{enumerate}
\url{https://proceedings.neurips.cc/paper_files/paper/2023/file/062d711fb777322e2152435459e6e9d9-Paper-Conference.pdf}


%Hvad er motivationen og formålet med projektet?



The methods of machine learning are a constantly evolving methodology as researchers are achieving continuously improving results on numerous tasks.  In the subject of biology, Google Deepmind has build AlphaFold 2 achieving incredible results on the protein folding problem \cite{jumper_highly_2021}. In the the area of Generative AI, OpenAI has constructed frameworks that shook the world, showcasing impressive models such as ChatGPT \cite{brown_language_2020} and DALL-E \cite{ramesh_zero-shot_2021}. However with all these advancements within data-science, comes a backside; an increase in ethical concerns. Something one could wonder about is, where is all this data coming from? How much of the data is person data? When the data is exploited it can have great consequences. For example, in the US election of 2016 the usage of personalised adds and targeted marketing, based on data collected on Facebook are believed to have had great relevance on the votes of US citizens. There is indeed no doubt that promoting privacy could be crucial in order to protect the well-being of citizens, as the earlier presented machine learning methods hold great power.  Several acts have already begun to limit the misuse of data, e.g. in Europa the General Data Protection Law (GDPR) has ever since 2018 protected the data of citizens in Europe. This project will especially be motivated by one of the principles stated in GDPR, which explicitly states that after a subject has given consent to provide data to some application, \textit{"the data subject shall have the right to withdraw his or her consent at any time "} \cite{noauthor_art_nodate}. 

\begin{comment}
\textit{The right to be forgotten} is something that has been introduced in 
\begin{itemize}
    \item European Union's General Data Protection Regulation (GDPR)
    \item The California Consumer Privacy Act (CCPA)
    \item The Act on the Protection of Personal Information (APPI)
    \item Canada's Consumer Privacy Protection Act (CPPA)
\end{itemize}
\end{comment}


%• Hvem har brug for resultaterne af projektet?
It seems reasonable that a subject should be able to regret their decision to share personal data. But consider the case where the data has been used to train a machine learning framework. Then each time a subject withdraws their data, the training data changes. The training data can usually easy be changed, however the trained model could potentially also hold information about the individual. The subject risk having their privacy breached as they could be victim to attacks such as \textit{membership inference attacks} \cite{shokri_membership_2017} or \textit{model inversion attacks} \cite{fredrikson_model_2015}. As a result of this, the model should be manipulated such that it mimics a resembling model that never has seen the data of the subject. The most aggressive way to achieve this is by retraining the whole model on the new training data. This is likely something that one would like to avoid if the model training is computational expensive. As an example consider the cost of training of the GPT-3, which is estimated to be around $12$ million dollars \cite{wiggers_openais_2020}. Several methods are available to avoid this. Examples include \textit{Differential privacy} [CITE] and \textit{Machine Unlearning} [CITE], where this project will consider the latter. By using it machine unlearning, it is sought to unlearn the relevant observations from the model, while not corrupting the performance of the framework. 



% Hvad er projektets baggrundsviden (state-of-the-art)?
% -> Litterature review



% Hvad er problemformuleringen/forskningsspørgsmålene?
%• Hvilken fremgangsmåde er anvendt til at løse problemstillingen?

% what do this project bring to the current state of the art?
%   application of machine unlearning in the domain of time series. 
%   application of machine unlearning on representation learned model






This project will as a starting point mimic the model created in the article \textit{"TS2Vec: Towards Universal Representation of Time Series"} \cite{yue_ts2vec_2022}. This model based on the concept of representation learning is then unlearned using different methods. \textcolor{red}{The following will be specified later in the project.}
\begin{itemize}
    \item Data Reorganisation
    \begin{itemize}
        \item Data obsfuscation
        \item Data pruning
    \end{itemize}
    \item Model Manipulation
    \begin{itemize}
        \item Amnesiac Unlearning
        \item Model Pruning
    \end{itemize}
\end{itemize}
% research questions
\begin{itemize}
    \item \textit{(to what degree it is possible) to learn meaningful features from time series data?}

    Ideally, the features hold important information about each of the data points, which could be used for other (downstream) tasks like prediction, interpolation or more (insert more). 

    \item \textit{To what degree is it possible to unlearn a complex model build on representation learning?}

    Optimally it should not be possible to see a difference in performance between the unlearned model and a model trained on the original dataset minus the unlearned data. 

    \item \textit{Do the different machine unlearning all ensure privacy?}

    It is sought, that the different methods tested all ensure the privacy of the citizen.

    \item \textit{Is the unlearned models robust towards attack methods such as membership inference and/or model inversion attacks?}

    This question resembles the above standing.

    \item \textit{Does the model still work on the unlearned samples?}

    It is sought that the individual observations should have worse performance, as the data that the individual bring along shouldn't be learned by the model? But isn't this just making sure the model isn't overfitted? 

    \item \textit{Is it more computational efficient to use the Machine Unlearning rather than retrain the whole model?}

    By comparing the computational resources used when training the whole model with the respective unlearning methods, it is a goal that the machine unlearning methods are faster and more efficient, therefore providing value. 

    \item \textit{Is the overall performance lowered?}

\end{itemize}



