\textcolor{red}{WARNING:} THIS IS TOO SIMILAR TO THE ARTICLE


The objective in representation learning is the following: given a set of N time series instances $\mathcal{X}=\{x_1, x_2, \dots, x_N\}$, learn a non-linear embedding function $f_\theta$ that maps from an observation to a representation $r_i$:
\[
f_\theta: x_i \to r_i
\]
The observation is of dimension $T \times F$, where $T$ is the sequence length, and $F$ is the feature dimension. The representation is of size $T$: $r_i = \{r_{i,1}, r_{i,2}, \dots, r_{i,T}\}$


\section{Contrastive loss}



\begin{definition}[Context]
    Is generated by applying random cropping and timestamp masking.
\end{definition}


\begin{definition}[Contextual Consistency]
    Two augmented contexts that are at the same timestamp are considered as a positive pair
\end{definition}

% benifits:
%   do not alter the amplitude of time seiries
%   enforce robustness
%   RC avoids *representation collapse*
There are multiple benefits from performing these augmentations. They do not alter the amplitude of the signals. This is important because [SOURCE]. Furthermore, they do enforce robustness... Specially, random cropping avoids representation collapse [SOURCE]. 

a
\section{Hiarcical Contrasting}
\begin{itemize}
    \item forces encoder to learn representations at different levels
    \item 
\end{itemize}





\begin{algorithm}
\caption{Hierarchical Loss (strongly inspired by algorithm 1 in \cite{yue_ts2vec_2022})}\label{alg:cap}
\SetKwProg{Fn}{def}{:}{}
\DontPrintSemicolon

 \SetKwFunction{FMain}{Main}
  \SetKwFunction{FSum}{HierLoss}
  \SetKwFunction{FSub}{Sub}
  

 \Fn{\FSum{$r$, $r'$}}{
        $\mathcal{L}_{\text{hier}}$ = $\mathcal{L}_{\text{dual}}(r,r')$\;
        $d$ = $1$\;
        \While{len(r) $>$ 1}
        {
        r = maxpool1d(r, kernelsize=2) \;
        r' = maxpool1d(r, kernelsize=2) \;
        $\mathcal{L}_{\text{hier}}$ = $\mathcal{L}_{\text{hier}}$ + $\mathcal{L}_{\text{dual}}(r,r')$ \;
        d = d + 1
        }
        $\mathcal{L}_{\text{hier}}$ = $\mathcal{L}_{\text{hier}}/d$ \;
        \KwRet $\mathcal{L}_{\text{hier}}$      
  }
\end{algorithm}






\[
\ell_{\text{temp}}^{(i,t)} = -\log{\frac{\exp{(r_{i,t} \cdot r_{i,t}')}}{\sum_{t'\in \Omega} \left( \exp{(r_{i,t} \cdot r_{i,t'}')} + \mathbb{I}_{[t\neq t']} \exp{(r_{i,t} \cdot r_{i,t'})}\right)}}
\]
$\Omega$ is the set of timestamps within the overlap of the two subseries. 

\[
\ell_{\text{inst}}^{(i,t)} = -\log{\frac{\exp{(r_{i,t} \cdot r_{i,t}')}}{\sum_{j=1}^B \left( \exp{(r_{i,t} \cdot r_{j,t}')} + \mathbb{I}_{[i\neq j]} \exp{(r_{i,t} \cdot r_{j,t})}\right)}}
\]
$B$ is batch size


