
@inproceedings{franceschi_unsupervised_2019,
	title = {Unsupervised Scalable Representation Learning for Multivariate Time Series},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/53c6de78244e9f528eb3e1cda69699bb-Abstract.html},
	abstract = {Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
	urldate = {2024-02-13},
	date = {2019},
	keywords = {{UNORGANISED}},
}

@misc{yue_ts2vec_2022,
	title = {{TS}2Vec: Towards Universal Representation of Time Series},
	url = {http://arxiv.org/abs/2106.10466},
	doi = {10.48550/arXiv.2106.10466},
	shorttitle = {{TS}2Vec},
	abstract = {This paper presents {TS}2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, {TS}2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, {TS}2Vec achieves significant improvement over existing {SOTAs} of unsupervised time series representation on 125 {UCR} datasets and 29 {UEA} datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous {SOTAs} of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes {SOTA} results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
	number = {{arXiv}:2106.10466},
	publisher = {{arXiv}},
	author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
	urldate = {2024-02-13},
	date = {2022-02-03},
	eprinttype = {arxiv},
	eprint = {2106.10466 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, {UNORGANISED}},
}

@misc{shaik_exploring_2024,
	title = {Exploring the Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy},
	url = {http://arxiv.org/abs/2305.06360},
	shorttitle = {Exploring the Landscape of Machine Unlearning},
	abstract = {Machine unlearning ({MU}) is gaining increasing attention due to the need to remove or modify predictions made by machine learning ({ML}) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of {MU}, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of {MU} and its future directions. Additionally, the paper emphasizes the need for researchers and practitioners to continue exploring and refining unlearning techniques to ensure that {ML} models can adapt to changing circumstances while maintaining user trust. The importance of unlearning is further highlighted in making Artificial Intelligence ({AI}) more trustworthy and transparent, especially with the increasing importance of {AI} in various domains that involve large amounts of personal user data.},
	number = {{arXiv}:2305.06360},
	publisher = {{arXiv}},
	author = {Shaik, Thanveer and Tao, Xiaohui and Xie, Haoran and Li, Lin and Zhu, Xiaofeng and Li, Qing},
	urldate = {2024-02-13},
	date = {2024-01-31},
	eprinttype = {arxiv},
	eprint = {2305.06360 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}
